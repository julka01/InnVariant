{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'disentangling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdisentangling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed_everything\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'disentangling'"
     ]
    }
   ],
   "source": [
    "from disentangling.metrics import *\n",
    "from experiments.utils.seed_everything import seed_everything\n",
    "import os\n",
    "import pickle\n",
    "from experiments.utils.imported_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRIBUTION = [np.random.uniform, {'low': 0., 'high': 1.}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factors_codes_decreasing_modularity_compactness(sample_size, num_factors = 6, noise_level= 0.0):\n",
    "    ''' Create factors-codes dataset\n",
    "\n",
    "    :param noise_level:     noise level to compute codes from factors\n",
    "    '''\n",
    "    # create factors dataset\n",
    "    dist, dist_kwargs = DISTRIBUTION\n",
    "    factors = get_artificial_factors_dataset(nb_examples=sample_size, nb_factors=num_factors,\n",
    "                                             distribution=dist, dist_kwargs=dist_kwargs)\n",
    "\n",
    "    # create projection matrix\n",
    "    projection = (1 - noise_level) * np.eye(num_factors) + noise_level * np.eye(num_factors, k=1)\n",
    "    projection[-1, 0] = noise_level\n",
    "\n",
    "    # compute codes from continuous factors\n",
    "    codes = np.dot(factors, projection)\n",
    "\n",
    "    return factors, codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(metrics, n_times, samples=[1000, 10000, 20000, 30000]):\n",
    "\n",
    "    results = {metric.__name__: [] for metric in metrics}  # Initialize results dictionary\n",
    "    base_path = \"results/experiment_complexity\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    for sample_size in samples:\n",
    "        factors, codes = get_factors_codes_decreasing_modularity_compactness(sample_size)\n",
    "        sample_results = {metric.__name__: [] for metric in metrics}  # Initialize sample results\n",
    "\n",
    "        for n in range(n_times):\n",
    "            seed_everything(n)  # Seed setting for reproducibility\n",
    "\n",
    "            for metric in metrics:\n",
    "                start_time = time.time()\n",
    "                res = metric(factors, codes)                  \n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                sample_results[metric.__name__].append(duration)  # Append duration for this run\n",
    "\n",
    "        # Calculate the average duration across all cases and seeds for each metric\n",
    "        for metric_name, durations in sample_results.items():\n",
    "            avg_duration = np.mean(durations)\n",
    "            results[metric_name].append(avg_duration)  # Corrected to append avg_duration directly\n",
    "\n",
    "    # Save the results for each metric in individual pickle files\n",
    "    for metric_name, avg_durations in results.items():\n",
    "        # Create a dictionary to be pickled with sample sizes as keys and avg_durations as values\n",
    "        result_dict = {sample_size: avg_duration for sample_size, avg_duration in zip(samples, avg_durations)}\n",
    "        with open(os.path.join(base_path, f'{metric_name}.pkl'), 'wb') as output_file:\n",
    "            pickle.dump(result_dict, output_file)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/envs/ubisoft/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [02:17:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics =  [dci_xgb, mig_ksg, dci, edi]\n",
    "for metric in metrics:\n",
    "    df = test_metrics(metrics = [metric], n_times = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(plots, output_file, format='png', figsize=(25, 6), x_scale='linear',\n",
    "                y_scale='linear', x_lims=None, y_lims=(-0.05, 1.05), x_label=\"\", y_label=\"\",\n",
    "                title_font_size=22, axis_font_size=16, legend_font_size=13, colors=None,\n",
    "                line_styles=None, legend_positions=None, legend_columns=None):\n",
    "    ''' Plot curves from a dictionary into the same figure\n",
    "\n",
    "    :param plots:                   {title: {curves_to_plot}} dictionary\n",
    "                                    the following format is assumed:\n",
    "                                    {\n",
    "                                        title_plot_1:\n",
    "                                        {\n",
    "                                            <legend_curve_1>: ([x0, x1, ..., xN], [y0, y1, ..., yN]),\n",
    "                                            ...\n",
    "                                        }\n",
    "                                        ...\n",
    "                                    }\n",
    "    :param output_file:             where and under which name the plot is saved\n",
    "    :param format:                  format to save the plot\n",
    "    :param figsize:                 size of the figure\n",
    "    :param x_scale:                 scale on x axis\n",
    "    :param y_scale:                 scale on y axis\n",
    "    :param x_lims:                  limit values on x axis\n",
    "    :param y_lims:                  limit values on y axis\n",
    "    :param x_label:                 label for the x axis\n",
    "    :param y_label:                 label for the y axis\n",
    "    :param title_font_size:         font size for title of each plot\n",
    "    :param axis_font_size:          font size of axis labels\n",
    "    :param legend_font_size:        font size of legend labels\n",
    "    :param colors:                  colors to use for the curves in the plots\n",
    "    :param line_styles:             line styles to use for the curves in the plot\n",
    "    :param legend_positions:        positioning of the legend for each plot\n",
    "    :param legend_columns:          number of columns in the legend for each plot\n",
    "    '''\n",
    "    _, axes = plt.subplots(nrows=1, ncols=len(plots), figsize=figsize)\n",
    "\n",
    "    for idx, title in enumerate(plots):\n",
    "        # extract legend labels\n",
    "        curves = plots[title]\n",
    "        legends = [legend for legend in curves]\n",
    "\n",
    "        # cycle through colors\n",
    "        colors_cycle = cycle(colors) if colors is not None else cycle(['blue', 'green', 'red'])\n",
    "        colors_cycle = [next(colors_cycle) for _ in range(len(curves))]\n",
    "\n",
    "        # cycle through line styles\n",
    "        lines_cycle = cycle(line_styles) if line_styles is not None else cycle(['-'])\n",
    "        lines_cycle = [next(lines_cycle) for _ in range(len(curves))]\n",
    "\n",
    "        # plot curves\n",
    "        ax = axes.ravel()[idx]\n",
    "        for datapoints, legend, color, line_style in zip(curves.values(), legends, colors_cycle, lines_cycle):\n",
    "            x, y = datapoints  # x: x-values, y: mean values, z: standard deviation\n",
    "            print(x, y)\n",
    "            ax.plot(x, y, label=legend, color=color, linestyle=line_style)\n",
    "\n",
    "#             # Calculate the lower and upper bounds for the standard deviation area\n",
    "#             lower_bound = y - z\n",
    "#             upper_bound = y + z\n",
    "\n",
    "#             # Add the shaded area\n",
    "#             ax.fill_between(x, lower_bound, upper_bound, color=color, alpha=0.4)  # Adjust alpha for transparency\n",
    "\n",
    "        # set scales on each axis\n",
    "        ax.set_xscale(x_scale)\n",
    "        ax.set_yscale(y_scale)\n",
    "\n",
    "        # set limits on x and y axis\n",
    "        if x_lims is not None:\n",
    "            ax.set_xlim(x_lims[0], x_lims[1])\n",
    "        if y_lims is not None:\n",
    "            ax.set_ylim(y_lims[0], y_lims[1])\n",
    "\n",
    "        # set title and labels on axis\n",
    "        ax.set_title(title, fontsize=title_font_size, pad=20)\n",
    "        ax.set_xlabel(x_label, fontsize=axis_font_size)\n",
    "        ax.set_ylabel(y_label, fontsize=axis_font_size, labelpad=10)\n",
    "\n",
    "        # set x-ticks and y-ticks font size\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(axis_font_size)\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(axis_font_size)\n",
    "\n",
    "            # set legends\n",
    "        if legend_positions is not None and legend_columns is not None:\n",
    "            ax.legend(handlelength=2, loc='lower center', bbox_to_anchor=legend_positions[idx],\n",
    "                      fontsize=legend_font_size, ncol=legend_columns[idx])\n",
    "        elif legend_positions is not None:\n",
    "            ax.legend(handlelength=2, loc='lower center', bbox_to_anchor=legend_positions[idx],\n",
    "                      fontsize=legend_font_size)\n",
    "        elif legend_columns is not None:\n",
    "            ax.legend(handlelength=2, loc='lower center', fontsize=legend_font_size, ncol=legend_columns[idx])\n",
    "        else:\n",
    "            ax.legend(handlelength=2, loc='lower center', fontsize=legend_font_size)\n",
    "\n",
    "    # save the plot\n",
    "    save_dir = os.path.dirname(output_file)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(f'{output_file}.{format}', bbox_inches='tight', format=format)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# # METRIC_DICT = {'dcimig': 'DCIMIG', 'dcii Mod': 'DCII Mod', 'dci_xgb Mod': 'DCI Mod', 'dci_xgb Comp': 'DCI Comp', 'dci_xgb Expl': 'DCI Expl', 'mig': 'MIG', 'mig_ksg': 'MIG-ksg', 'mig_sup': 'MIG-sup', 'mig_sup_ksg': 'MIG-sup-ksg', 'dcimig_ksg': 'dcimig-ksg', 'z_min_var': 'FactorVAE', 'sap_xgb': 'SAP', 'modularity': 'Modularity'}\n",
    "# METRIC_DICT = {'dcimig': 'DCIMIG', 'dcii Mod': 'EDI Mod', 'dcii Comp': 'EDI Comp', 'dcii Expl':'EDI Expl', 'dci_xgb Mod': 'DCI Mod', 'dci_xgb Comp': 'DCI Comp', 'dci_xgb Expl': 'DCI Expl', 'mig': 'MIG', 'mig_ksg': 'MIG-ksg', 'mig_sup': 'MIG-sup', 'mig_sup_ksg': 'MIG-sup-ksg', 'dcimig_ksg': 'dcimig-ksg', 'z_min_var': 'FactorVAE', 'sap_xgb': 'SAP', 'modularity': 'Modularity'}\n",
    "\n",
    "# # config parameters for plots\n",
    "# loosely_dashed = (0, (5, 10))\n",
    "# densely_dashdotdotted = (0, (3, 1, 1, 1, 1, 1))\n",
    "# densely_dotted = (0, (1, 1))\n",
    "# PLOTS = {\n",
    "\n",
    "\n",
    "#     'COLORS': ['blue', 'green', 'red', 'darkturquoise', 'magenta', 'orange', 'black', 'violet', 'maroon'],\n",
    "#     'LINE STYLES': ['--', '-.', loosely_dashed, densely_dashdotdotted, ':', densely_dotted],\n",
    "#     'LEGEND POSITIONS': [(0.5, -0.2)],\n",
    "#     'NB LEGEND COLUMNS': [ 5, 5]\n",
    "# }\n",
    "\n",
    "METRIC_DICT_complexity = {dci_xgb: 'DCI-xgb', dcimig: 'DCIMIG', edi: 'EDI', dci: 'DCI', mig: 'MIG', mig_ksg: 'MIG-ksg', mig_sup: 'MIG-sup', mig_sup_ksg: 'MIG-sup-ksg', dcimig_ksg: 'dcimig-ksg', z_min_var: 'FactorVAE', sap: 'SAP', modularity: 'Modularity'}\n",
    "METRIC_DICT = {'dcimig': 'DCIMIG', 'dcii Mod': 'EDI Mod', 'dcii Comp': 'EDI Comp', 'dcii Expl':'EDI Expl', 'dci_xgb Mod': 'DCI Mod', 'dci_xgb Comp': 'DCI Comp', 'dci_xgb Expl': 'DCI Expl', 'mig': 'MIG', 'mig_ksg': 'MIG-ksg', 'mig_sup': 'MIG-sup', 'mig_sup_ksg': 'MIG-sup-ksg', 'dcimig_ksg': 'dcimig-ksg', 'z_min_var': 'FactorVAE', 'sap_xgb': 'SAP', 'modularity': 'Modularity'}\n",
    "\n",
    "loosely_dashed = (0, (5, 10))\n",
    "densely_dashdotdotted = (0, (3, 1, 1, 1, 1, 1))\n",
    "densely_dotted = (0, (1, 1))\n",
    "dash_dotted = (0, (3, 5, 1, 5))\n",
    "long_dashed = (0, (5, 5))\n",
    "short_dashed = (0, (5, 1))\n",
    "\n",
    "PLOTS = {\n",
    "    \n",
    "        'FAMILIES': {\n",
    "        'Metrics': [ 'mig'],\n",
    "      \n",
    "        \n",
    "    },\n",
    "    'COLORS': [\n",
    "        'blue', 'green', 'red', 'darkturquoise', 'magenta', \n",
    "        'orange', 'black', 'violet', 'maroon', 'lightgreen', \n",
    "        'mint', 'cyan', 'brown', 'pink', 'lime', 'yellow'\n",
    "    ],\n",
    "    'LINE STYLES': [\n",
    "        '--', '-.', loosely_dashed, densely_dashdotdotted, ':', \n",
    "        densely_dotted, dash_dotted, long_dashed, short_dashed, \n",
    "        (0, (1, 10)), (0, (5, 10)), (0, (3, 10, 1, 10))\n",
    "    ],\n",
    "    'LEGEND POSITIONS': [(0.5, -0.15)],\n",
    "    'NB LEGEND COLUMNS': [5]  # Adjusted for improved layout\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_scores():\n",
    "#     output_dir = \"results/experiment_complexity/\"\n",
    "#     scores = {}\n",
    "\n",
    "#     # Load metric scores from pickle files\n",
    "#     metric_files = [f for f in os.listdir(output_dir) if f.endswith('.pkl')]\n",
    "#     for file in metric_files:\n",
    "#         metric_name = os.path.splitext(file)[0]  # Get the file name without the extension\n",
    "#         with open(os.path.join(output_dir, file), 'rb') as f:\n",
    "#             metric_scores = pickle.load(f)\n",
    "#         scores[metric_name] = metric_scores\n",
    "\n",
    "#     # Initialize a figure for plotting\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     # A counter to keep track of which metric we're plotting for color and line style\n",
    "#     metric_counter = 0\n",
    "\n",
    "#     # Preparing data for plotting\n",
    "#     for metric_name, metric_scores in scores.items():\n",
    "#         print(metric_scores)\n",
    "#         sample_sizes = list(metric_scores.keys())\n",
    "#         values = list(metric_scores.values())\n",
    "#         display_name = METRIC_DICT[globals()[metric_name]] # Get the display name\n",
    "#         color = PLOTS['COLORS'][metric_counter % len(PLOTS['COLORS'])]\n",
    "#         line_style = PLOTS['LINE STYLES'][metric_counter % len(PLOTS['LINE STYLES'])]\n",
    "\n",
    "#         plt.plot(sample_sizes, values, label= display_name, color=color, linestyle=line_style)\n",
    "#         metric_counter += 1\n",
    "\n",
    "#     # Customizing the plot\n",
    "#     plt.xlabel('Sample Size')\n",
    "#     plt.ylabel('Duration (seconds)')\n",
    "#     plt.title('Time Complexity Analysis')\n",
    "#     plt.xscale('log')  # Applying logarithmic scale\n",
    "#     plt.xlim(left=1000, right=25000)\n",
    "#     plt.ylim(bottom = 0, top = 2000)\n",
    "#     # Adjust xlim and ylim as necessary, based on your data\n",
    "\n",
    "#     # Customizing legend with positions and columns\n",
    "#     legend_position = PLOTS['LEGEND POSITIONS'][0]\n",
    "#     nb_legend_columns = PLOTS['NB LEGEND COLUMNS'][0]\n",
    "#     plt.legend(loc='upper center', bbox_to_anchor=legend_position, ncol=nb_legend_columns)\n",
    "\n",
    "#     # Save and show the plot\n",
    "#     plt.savefig(os.path.join(output_dir, 'experiment_complexity.png'))\n",
    "#     # plt.savefig(os.path.join(output_dir, 'experiment_complexity.eps'))\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (740044962.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[121], line 38\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.legend(loc='upper center', bbox_to_anchor=bbox_to_anchor=(0.5, -0.15), ncol=5, fontsize=legend_font_size)  # Adjust legend font size\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def plot_scores(output_dir=\"results/experiment_complexity/\"):\n",
    "    scores = {}\n",
    "\n",
    "    # Load metric scores from pickle files\n",
    "    metric_files = [f for f in os.listdir(output_dir) if f.endswith('.pkl')]\n",
    "    for file in metric_files:\n",
    "        metric_name = os.path.splitext(file)[0]  # Get the file name without the extension\n",
    "        with open(os.path.join(output_dir, file), 'rb') as f:\n",
    "            metric_scores = pickle.load(f)\n",
    "        scores[metric_name] = metric_scores\n",
    "\n",
    "    # Initialize a figure for plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metric_counter = 0\n",
    "\n",
    "    # Preparing data for plotting\n",
    "    for metric_name, metric_scores in scores.items():\n",
    "        sample_sizes = list(metric_scores.keys())\n",
    "        values = list(metric_scores.values())\n",
    "        display_name = METRIC_DICT[globals()[metric_name]]  # Get the display name\n",
    "        color = PLOTS['COLORS'][metric_counter % len(PLOTS['COLORS'])]\n",
    "        line_style = PLOTS['LINE STYLES'][metric_counter % len(PLOTS['LINE STYLES'])]\n",
    "\n",
    "        plt.plot(sample_sizes, values, label=display_name, color=color, linestyle=line_style)\n",
    "        metric_counter += 1\n",
    "    # Customizing the plot\n",
    "    plt.xlabel('Sample Size', fontsize=axis_font_size)  # Adjust label font size\n",
    "    plt.ylabel('Duration (seconds)', fontsize=axis_font_size)  # Adjust label font size\n",
    "    plt.title('Time Complexity Analysis', fontsize=title_font_size)  # Adjust title font size\n",
    "    plt.xscale('log')  # Applying logarithmic scale\n",
    "    plt.xlim(left=1000, right=25000)\n",
    "    plt.ylim(bottom=0, top=2000)\n",
    "    # Adjust xlim and ylim as necessary, based on your data\n",
    "\n",
    "    # Customizing legend with positions and columns\n",
    "    legend_position = PLOTS['LEGEND POSITIONS'][0]\n",
    "    nb_legend_columns = PLOTS['NB LEGEND COLUMNS'][0]\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=bbox_to_anchor=(0.5, -0.15), ncol=5, fontsize=legend_font_size)  # Adjust legend font size\n",
    "\n",
    "    # Save and show the plot\n",
    "    plt.savefig(os.path.join(output_dir, 'experiment_complexity.png'))\n",
    "    # plt.savefig(os.path.join(output_dir, 'experiment_complexity.eps'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<function dci_xgb at 0x7fa0ca08fa60>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 20\u001b[0m, in \u001b[0;36mplot_scores\u001b[0;34m(output_dir)\u001b[0m\n\u001b[1;32m     18\u001b[0m sample_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(metric_scores\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     19\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(metric_scores\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m---> 20\u001b[0m display_name \u001b[38;5;241m=\u001b[39m \u001b[43mMETRIC_DICT\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Get the display name\u001b[39;00m\n\u001b[1;32m     21\u001b[0m color \u001b[38;5;241m=\u001b[39m PLOTS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOLORS\u001b[39m\u001b[38;5;124m'\u001b[39m][metric_counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(PLOTS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOLORS\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m     22\u001b[0m line_style \u001b[38;5;241m=\u001b[39m PLOTS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLINE STYLES\u001b[39m\u001b[38;5;124m'\u001b[39m][metric_counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(PLOTS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLINE STYLES\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n",
      "\u001b[0;31mKeyError\u001b[0m: <function dci_xgb at 0x7fa0ca08fa60>"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DCI_RF'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRIC_DICT[globals()['dci_xgb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (320525269.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[54], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    globals()'dci'\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "globals()'dci'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_font_size = 20\n",
    "axis_font_size = 18\n",
    "legend_font_size = 14\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_scores1(output_dir=\"results/experiment_sample_efficiency/\"):\n",
    "    scores = {}\n",
    "\n",
    "    # Load metric scores from pickle files\n",
    "    metric_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith('.pkl')]\n",
    "    for metric_file in metric_files:\n",
    "        with open(metric_file, 'rb') as f:\n",
    "            metric_scores = pickle.load(f)\n",
    "        scores.update(metric_scores)\n",
    "    \n",
    "    # Initialize a figure for plotting\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6))  # Create a grid of 1 row and 2 columns for the subplots\n",
    "\n",
    "    # Plotting the first figure\n",
    "    metric_counter = 0\n",
    "    for metric_name, (sample_sizes, values) in scores.items():\n",
    "        display_name = METRIC_DICT.get(metric_name, metric_name)\n",
    "        color = PLOTS['COLORS'][metric_counter % len(PLOTS['COLORS'])]\n",
    "        line_style = PLOTS['LINE STYLES'][metric_counter % len(PLOTS['LINE STYLES'])]\n",
    "        \n",
    "        ax[0].plot(sample_sizes, values, label=display_name, color=color, linestyle=line_style, linewidth = 2.0)\n",
    "        metric_counter += 1\n",
    "\n",
    "    # Customizing the first plot\n",
    "    ax[0].set_xlabel('Sample Size', fontsize=axis_font_size)\n",
    "    ax[0].set_ylabel('Score', fontsize=axis_font_size)\n",
    "    ax[0].set_title('Sample Efficiency', fontsize=title_font_size)\n",
    "    ax[0].set_xscale('log')\n",
    "    ax[0].set_xlim(left=100, right=100000)\n",
    "    ax[0].set_ylim(bottom=0, top=0.35)\n",
    "    ax[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=4, fontsize=legend_font_size)\n",
    "\n",
    "    # Plotting the second figure\n",
    "    scores = {}\n",
    "    output_dir = \"results/experiment_complexity/\"\n",
    "    metric_files = [f for f in os.listdir(output_dir) if f.endswith('.pkl')]\n",
    "    for file in metric_files:\n",
    "        metric_name = os.path.splitext(file)[0]\n",
    "        with open(os.path.join(output_dir, file), 'rb') as f:\n",
    "            metric_scores = pickle.load(f)\n",
    "        scores[metric_name] = metric_scores\n",
    "\n",
    "    metric_counter = 0\n",
    "    for metric_name, metric_scores in scores.items():\n",
    "        sample_sizes = list(metric_scores.keys())\n",
    "        values = list(metric_scores.values())\n",
    "        display_name = METRIC_DICT_complexity[globals()[metric_name]]\n",
    "        color = PLOTS['COLORS'][metric_counter % len(PLOTS['COLORS'])]\n",
    "        line_style = PLOTS['LINE STYLES'][metric_counter % len(PLOTS['LINE STYLES'])]\n",
    "\n",
    "        ax[1].plot(sample_sizes, values, label=display_name, color=color, linestyle=line_style, linewidth = 2.0)\n",
    "        metric_counter += 1\n",
    "\n",
    "    # Customizing the second plot\n",
    "\n",
    "    ax[1].set_xlabel('Sample Size', fontsize=axis_font_size)\n",
    "    ax[1].set_ylabel('Duration (seconds)', fontsize=axis_font_size)\n",
    "    ax[1].set_title('Time Complexity', fontsize=title_font_size)\n",
    "    ax[1].set_xscale('log')\n",
    "    ax[1].set_xlim(left=1000, right=25000)\n",
    "    ax[1].set_ylim(bottom=0, top=2000)\n",
    "    ax[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=4, fontsize=legend_font_size)\n",
    "\n",
    "    # Save and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'experiment_combined_efficiency_complexity.png'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/experiment_sample_efficiency/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_scores1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m, in \u001b[0;36mplot_scores1\u001b[0;34m(output_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load metric scores from pickle files\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m metric_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric_file \u001b[38;5;129;01min\u001b[39;00m metric_files:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(metric_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/experiment_sample_efficiency/'"
     ]
    }
   ],
   "source": [
    "plot_scores1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ubisoft",
   "language": "python",
   "name": "ubisoft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
